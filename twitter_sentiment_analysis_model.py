# -*- coding: utf-8 -*-
"""Full Dataset Done Twitter Sentiment analysis model.ipynb 4/4/22

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yFIGZNAk_g1kqU1URPsLmi1Dwr_GaBJF
"""


#libraries
import tweepy
import os
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import math
import numpy as np
import requests
from datetime import datetime
from time import time
from numpy.core.arrayprint import str_format
import re
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import io
import matplotlib.pyplot as plt
from google.colab import files
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import tensorflow as tf
from tensorflow import keras
from keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM
from tensorflow.python.keras.layers import CuDNNLSTM
from cryptocmd import CmcScraper
from dotenv import load_dotenv




plt.style.use('fivethirtyeight')

from bs4 import BeautifulSoup 
import time
def bs4_realtimeprice(coin):
  url = "https://www.google.com/search?q=" + coin + "+price"
  HTML = requests.get(url) 
  #Parse the HTML
  soup = BeautifulSoup(HTML.text, 'html.parser') 
  text = soup.find("div", attrs={'class':'BNeawe iBp4i AP7Wnd'}).find("div", attrs={'class':'BNeawe iBp4i AP7Wnd'}).text
  return text
load_dotenv()
#Twitter API credentials
consumerKey = os.getenv("CONSUMERKEY")
consumerSecret = os.getenv("CONSUMERSECRET")
accessToken = os.getenv("ACCESSTOKEN")
accessTokenSecret = os.getenv("ACCESSTOKENSECRET")

#Create the authentication object
authenticate = tweepy.OAuthHandler(consumerKey,consumerSecret)


#set the access token and access token secret

authenticate.set_access_token(accessToken, accessTokenSecret)

#create API object while passing in the auth info
api = tweepy.API(authenticate, wait_on_rate_limit = True)

# Extract tweets from bitcoin search query
# "since" parameter exists
#replace so we do not need to hard code
day = 24
month = 5
posts_total = []

for x in range(8):
  if day < 31:
    posts = api.search(q = "bitcoin", count = 100, lang = "en", result_type = 'mixed', since = "2022-0" + str(month) +"-" + str(day), until = "2022-0" + str(month) +"-" + str(day+1), tweet_mode = 'extended')
    for tweet in posts[0:70]:
      since_date =  "2022-0"+ str(month) +"-" + str(day)
      posts_total.append([since_date.split(" ")[0], tweet.full_text])
    day += 1
  elif day == 31:
    print("correct")
    posts = api.search(q = "bitcoin", count = 100, lang = "en", result_type = 'mixed', since = "2022-0" + str(month) +"-" + str(day), until = "2022-0" + str(month+1) +"-" + str(1), tweet_mode = 'extended')
    for tweet in posts[0:70]:
      since_date =  "2022-0"+ str(month) +"-" + str(day)
      posts_total.append([since_date.split(" ")[0], tweet.full_text])
    day = 1
    month += 1  
  else:
    day = 1
    month += 1
    posts = api.search(q = "bitcoin", count = 100, lang = "en", result_type = 'mixed', since = "2022-0" + str(month) +"-" + str(day), until = "2022-0" + str(month) +"-" + str(day+1), tweet_mode = 'extended')
    for tweet in posts[0:70]:
      since_date =  "2022-0"+ str(month) +"-" + str(day)
      posts_total.append([since_date.split(" ")[0], tweet.full_text])
    day+=1


  """
 if day <= 31:
  posts = api.search(q = "bitcoin", count = 70, lang = "en", result_type = 'mixed', since = "2022-0" + str(month) +"-" + str(day), until = "2022-0" + str(month) +"-" + str(day+1), tweet_mode = 'extended')
  for tweet in posts[0:70]:
    since_date =  "2022-0"+ str(month) +"-" + str(day)
    #posts_total.append([since_date.split(" ")[0], tweet.full_text])
    posts_total.append([tweet.created_at, tweet.full_text])
    day += 1
 else:
    day = 1
    month += 1
    posts = api.search(q = "bitcoin", count = 70, lang = "en", result_type = 'mixed', since = "2022-0" + str(month) +"-" + str(day), until = "2022-0" + str(month) +"-" + str(day+1), tweet_mode = 'extended')
    for tweet in posts[0:70]:
      since_date =  "2022-0"+ str(month) +"-" + str(day)
      #posts_total.append([since_date.split(" ")[0], tweet.full_text])
      posts_total.append([tweet.created_at, tweet.full_text])
"""

#posts = api.search(q = "bitcoin", count = 100, lang = "en", result_type = 'mixed', since = "2022-02-0" + str(day), until = "2022-02-0" + str(day + 1), tweet_mode = 'extended')

#dfTotal = pd.DataFrame([posts_total[0]], columns=['Tweet', 'Tweet'])
#dfTotal

#generate posts array that includes tweets from a 1-day span
#using getTime, stack posts array with the times of each tweet in that array
#using a for loop, check with split[" "]  to see the first two digits of the second split to check the hour
#if hour is +1, then split the array there

#print 8 most recent tweets until ______
#for tweet in posts_total[0:600]:
  #print(tweet)

#Create dataframe with "Tweet" column
#df = pd.DataFrame(, columns=['Tweets'])
#df.insert(0, "Date", posts_total[0] , True)
df = pd.DataFrame(posts_total)
df.columns = ['Date','Tweet']



#Show first 8 rows of data
df.head(8)

#Cleansing tweets function
def data_Cleaning(tweets):
  tweets = re.sub(r'@[A-Za-z0-9]', '', tweets) #removing mentions
  tweets = re.sub(r'#', '', tweets)
  tweets = re.sub(r'https?:\/\/\S+', '', tweets)
  tweets = re.sub(r"RT[\s]", '', tweets)
  tweets = re.sub(r"Retweet[\s]", '', tweets)
  tweets = re.sub(r"Follow[\s]", '', tweets)
  tweets = re.sub(u"\U0001F600-\U0001F64F", '', tweets) #default emotes/emojis
  tweets = re.sub(u"\U0001F300-\U0001F5FF", '', tweets) #default symbols/pictographs
  tweets = re.sub(u"\U0001F680-\U0001F6FF", '', tweets) #transportation symbols
  tweets = re.sub(u"\U0001F1E0-\U0001F1FF", '', tweets) #flags (ios)
  
  return tweets
def emoji_Cleaning(tweets):
  emoji_pattern = re.compile(
      "["
      u"\U0001F600-\U0001F64F"
      u"\U0001F300-\U0001F5FF"
      u"\U0001F680-\U0001F6FF" 
      u"\U0001F680-\U0001F6FF"   
      u"\U0001F1E0-\U0001F1FF"
      "]+",
      flags=re.UNICODE,  
  )
  return emoji_pattern.sub(r"", tweets)
#clean & print the text
df['Tweet'] = df['Tweet'].apply(data_Cleaning)
df['Tweet'] = df['Tweet'].apply(emoji_Cleaning)

df

analyzer = SentimentIntensityAnalyzer()
df['neg'] = df['Tweet'].apply(lambda x:analyzer.polarity_scores(x)['neg'])
df['neu'] = df['Tweet'].apply(lambda x:analyzer.polarity_scores(x)['neu'])
df['pos'] = df['Tweet'].apply(lambda x:analyzer.polarity_scores(x)['pos'])
df['compound'] = df['Tweet'].apply(lambda x:analyzer.polarity_scores(x)['compound'])

#show data
df

#Average Compound Values
Start_Date = df.iloc[0,0]
count = 0
Sum = 0
Compound_Averages = []
Dataset_Dates = []
Dataset_Dates.append(Start_Date)
for index, row in df.iterrows():
    if(row['Date'] == Start_Date):
      Sum+= float(row['compound'])
      count+=1
    else:
      Compound_Averages.append((Sum/count))
      Start_Date = row['Date']
      Dataset_Dates.append(Start_Date)
      Sum = 0
      count = 0
#Create Data Frame for final Values to Be Appended
Compound_Averages.append((Sum/count))
Dataset = pd.DataFrame(Dataset_Dates)
Dataset['Polarity'] = Compound_Averages
Dataset.columns = ['Date','Polarity']
Dataset

#Get bitcoin prices - Delta Price, Open, Close, High, Low
'''
base_url = "https://api.kucoin.com"
coin_pair = "BTC-USDT" #BTC-USDTo
frequency = "1day" 


now_is = int(time()) - (60*60*24*2)#get timestamp date of today in seconds
print(now_is)
days = 7
days_delta = 60 * 60 * 24 * days 
start_At = now_is - days_delta
price_url = f"/api/v1/market/candles?type={frequency}&symbol={coin_pair}&startAt={start_At}&endAt={now_is}"

prices = requests.get(base_url+price_url).json()
print(prices) #entire dict of prices for 7days
price_arr = []
'''

'''
prices = requests.get(base_url+price_url).json()
for item in prices['data']:
  #convert date from timestamp to Y M D
  date_converted = datetime.fromtimestamp(int(item[0])).strftime("%m-%d-%Y")
  price_arr.append(item[2])

#add prices to dataframe
btc_close_arr = []
close_arr = []
count = 1
lastval = len(price_arr)
#parsing for only closing price


print(price_arr)
Dataset['BTC_Price'] = price_arr[len(price_arr)-9:len(price_arr)-1]
display(Dataset)
#priceDF = pd.DataFrame(price_dict,index=["Bitcoin Price"]).T
#priceDF["Bitcoin Price"] = priceDF["Bitcoin Price"].astype(float)

#convert dates to datetime from object

#reverse dates
#priceDF = priceDF.iloc[::-1]
#priceDF.columns = ['Date','BTC_Price']
#print(priceDF)
#append dataframes
#extracted_col = priceDF['Bitcoin Price']
#display(extracted_col)
#Dataset.join(priceDF)
#print(Dataset)
'''
arr = []
for date in Dataset_Dates:
  tempDate = str(date)
  date = tempDate
  date = date.replace('2022-', '')
  date += '-2022'
  mid = date[0:3]
  date = date[3:]
  index = date.find('-')
  beg = date[0:index+1]
  end = date[index+1:]
  date = beg + mid+ end
  arr.append(date)
scraper = CmcScraper("BTC", arr[0], arr[7])
# Pandas dataFrame for the same data
tempPrices = scraper.get_dataframe()
tempPrices = tempPrices.iloc[::-1]
Dataset["Open"] = tempPrices["Open"]
Dataset["High"] = tempPrices["High"]
Dataset["Low"] = tempPrices["Low"]
Dataset["Volume"] = tempPrices["Volume"]
Dataset["Market Cap"] = tempPrices["Market Cap"]


Dataset["Close"] = tempPrices["Close"]

Dataset

model_dataframe = Dataset.iloc[:,1:]
model_dataframe

#Start of LSTM

"""
model_dataframe = pd.DataFrame(Dataset['Polarity'])
model_dataframe['BTC_Price'] = price_arr[len(price_arr)-9:len(price_arr)-1]

#Create Dataset

model_dataset = model_dataframe.values
model_dataset = model_dataset.astype('float32')
#print(model_dataset)


#Convert to dataset matrix method
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)
 
look_back = 1

#Split into Train and Test Data
train = model_dataset[0:5,:]
test = model_dataset[5:8,:]
#print(test)

#reshape matrix into x = t and y = t+1
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)

trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

#Create LSTM
model = Sequential()
model.add(LSTM(4, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=5, verbose=2)


#generate predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)

#plot data
trainPredictPlot = np.empty_like(model_dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(model_dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(model_dataset)-1, :] = testPredict
# plot baseline and predictions
plt.plot(model_dataset)
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()
"""



#Start of LSTM
#Create Dataset
model_dataset = model_dataframe.values
model_dataset = model_dataset.astype('float32')
#print(model_dataset)

#Normalization
scaler = MinMaxScaler()

close_price = model_dataframe.Close.values.reshape(-1, 1)

scaled_close = scaler.fit_transform(close_price)

#print(scaled_close.shape)
#print(np.isnan(scaled_close).any())

scaled_close = scaled_close[~np.isnan(scaled_close)]

scaled_close = scaled_close.reshape(-1, 1)

#print(np.isnan(scaled_close).any())

#Preprocessing

SEQ_LEN = 2

def to_sequences(data, seq_len):
    d = []

    for index in range(len(data) - seq_len):
        d.append(data[index: index + seq_len])

    return np.array(d)

def preprocess(data_raw, seq_len, train_split):

    data = to_sequences(data_raw, seq_len)

    #num_train = int(train_split * data.shape[0])

    X_train = data[:5, :-1, :]
    y_train = data[:5, -1, :]

    X_test = data[5:, :-1, :]
    y_test = data[5:, -1, :]

    return X_train, y_train, X_test, y_test

X_train, y_train, X_test, y_test = preprocess(scaled_close, SEQ_LEN, train_split = 0.875)

print(X_train.shape)


print(X_test.shape)


#model

DROPOUT = 0.2
WINDOW_SIZE = SEQ_LEN - 1

model = keras.Sequential()

model.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=True),input_shape=(WINDOW_SIZE, X_train.shape[-1])))
model.add(Dropout(rate=DROPOUT))
model.add(Bidirectional(LSTM((WINDOW_SIZE * 2), return_sequences=True)))
model.add(Dropout(rate=DROPOUT))
model.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=False)))
model.add(Dense(units=1))
model.add(Activation('linear'))

model.compile(
    loss='mean_squared_error', 
    optimizer='adam'
)
BATCH_SIZE = 6
history = model.fit(
    X_train, 
    y_train, 
    epochs=200, 
    batch_size=BATCH_SIZE, 
    shuffle=False,
    validation_split=0.1
)

model.evaluate(X_test, y_test)


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


y_hat = model.predict(X_test)

y_test_inverse = scaler.inverse_transform(y_test)
y_hat_inverse = scaler.inverse_transform(y_hat)

print("Predicted Price: " , y_hat_inverse)
print("Actual Price: ", bs4_realtimeprice('Bitcoin'))
plt.show()

#obtaining real time price using google query and comparing it to predicted value
btcrt_price = bs4_realtimeprice("Bitcoin").split(" ")[0]
btcrt_price = btcrt_price[0:2] + btcrt_price[3:]
p_error = ((y_hat_inverse - float(btcrt_price))/ (y_hat_inverse))*100
print("Percent Error: ", p_error, "%")